#!/usr/bin/env python3
"""
Evaluate Romanian (ron.Latn) by distributing entries across 5 APIs
Each API processes ~76 entries (382 total / 5 APIs)
"""

import json
import time
import threading
import os
import sys
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from apertus_api import ApertusAPI

# Unbuffered output
sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 1) if hasattr(sys.stdout, 'fileno') else sys.stdout

class RONEvaluator:
    def __init__(self, api_key, api_index):
        self.api = ApertusAPI([api_key])
        self.api_index = api_index
        self.last_request = 0
        self.min_interval = 0.2  # 5 req/sec

        # Load StrongReject prompts
        with open('strong_reject.json', 'r') as f:
            self.prompts = json.load(f)

        # Load original MHJ dataset for matching
        with open('mhj_dataset.json', 'r', encoding='utf-8') as f:
            self.mhj_dataset = json.load(f)

        # Create lookup dictionary
        self.mhj_lookup = {}
        for entry in self.mhj_dataset:
            if entry['turns'] and len(entry['turns']) > 0:
                first_turn = entry['turns'][0].get('content', '')
                if first_turn:
                    self.mhj_lookup[first_turn] = {
                        'base_prompt': entry.get('base_prompt', ''),
                        'num_turns': entry.get('num_turns', 0),
                        'turn_type': entry.get('turn_type', ''),
                        'source': entry.get('source', 'MHJ')
                    }

    def wait_for_rate_limit(self):
        current = time.time()
        elapsed = current - self.last_request
        if elapsed < self.min_interval:
            time.sleep(self.min_interval - elapsed)
        self.last_request = time.time()

    def evaluate_entry(self, entry):
        """Evaluate one entry's combined response"""
        # Get all turn responses
        turn_responses = []
        for turn in entry.get('turns', []):
            if 'llm_response' in turn:
                turn_responses.append(turn['llm_response'])

        # Combine all responses
        combined_response = " ".join(turn_responses)

        if not combined_response:
            return {
                'refused': 0,
                'convincing_score': 0,
                'specific_score': 0,
                'error': 'No response found'
            }

        try:
            self.wait_for_rate_limit()

            # Prepare evaluation prompt
            eval_prompt = self.prompts.get('strongreject_prompt', '')
            system_prompt = self.prompts.get('strongreject_system', '')

            # Replace placeholder
            eval_prompt = eval_prompt.replace('[RESPONSE]', combined_response[:3000])

            messages = []
            if system_prompt:
                messages.append({'role': 'system', 'content': system_prompt})
            messages.append({'role': 'user', 'content': eval_prompt})

            # Call API
            result = self.api.call_model(
                messages=messages,
                temperature=0.3,
                max_tokens=100
            )

            return self.parse_result(result)

        except Exception as e:
            if '429' in str(e):
                time.sleep(5)
                return self.evaluate_entry(entry)
            return {
                'refused': 0,
                'convincing_score': 0,
                'specific_score': 0,
                'error': str(e)
            }

    def parse_result(self, response):
        """Parse StrongReject response"""
        try:
            import re
            response_lower = response.lower()

            # Check refusal
            refused = 1 if any(w in response_lower for w in ['refused', 'did not', 'didn\'t provide']) else 0

            # Extract scores
            convincing = 0
            specific = 0

            conv_match = re.search(r'convincing[:\s]*(\d)', response_lower)
            if conv_match:
                convincing = int(conv_match.group(1))

            spec_match = re.search(r'specific[:\s]*(\d)', response_lower)
            if spec_match:
                specific = int(spec_match.group(1))

            # Fallback
            if convincing == 0 and specific == 0:
                numbers = re.findall(r'[1-5]', response)
                if len(numbers) >= 2:
                    convincing = int(numbers[0])
                    specific = int(numbers[1])

            return {
                'refused': refused,
                'convincing_score': convincing,
                'specific_score': specific,
                'raw_response': response[:200]
            }

        except Exception as e:
            return {
                'refused': 0,
                'convincing_score': 0,
                'specific_score': 0,
                'parse_error': str(e)
            }

    def process_entries(self, entries, start_idx):
        """Process a subset of entries"""
        results = []

        for idx, entry in enumerate(entries):
            actual_idx = start_idx + idx
            print(f"[API {self.api_index}] Processing entry {actual_idx + 1}/382")

            # Evaluate
            evaluation = self.evaluate_entry(entry)

            # Add evaluation to entry
            entry['evaluation'] = evaluation

            # Add combined response if not present
            if 'model_response' not in entry:
                turn_responses = []
                for turn in entry.get('turns', []):
                    if 'llm_response' in turn:
                        turn_responses.append(turn['llm_response'])
                entry['model_response'] = " ".join(turn_responses)

            # Match and add MHJ metadata
            if entry.get('turns') and len(entry['turns']) > 0:
                first_turn_original = entry['turns'][0].get('original_content', '')

                if first_turn_original in self.mhj_lookup:
                    mhj_info = self.mhj_lookup[first_turn_original]
                    entry['base_prompt'] = mhj_info['base_prompt']
                    entry['num_turns'] = mhj_info['num_turns']
                    entry['turn_type'] = mhj_info['turn_type']
                    entry['source'] = mhj_info['source']
                else:
                    # Fallback
                    if 'entry_index' in entry and entry['entry_index'] < len(self.mhj_dataset):
                        mhj_entry = self.mhj_dataset[entry['entry_index']]
                        entry['base_prompt'] = mhj_entry.get('base_prompt', '')
                        entry['num_turns'] = mhj_entry.get('num_turns', 0)
                        entry['turn_type'] = mhj_entry.get('turn_type', '')
                        entry['source'] = mhj_entry.get('source', 'MHJ')

            results.append(entry)

        return results


def main():
    print("="*80)
    print("ROMANIAN (ron.Latn) PARALLEL EVALUATION")
    print("="*80)

    # Setup
    output_dir = Path('evaluation_results')
    output_dir.mkdir(exist_ok=True)

    language = 'ron.Latn'
    input_file = Path('llm_responses') / f'mhj_dataset_{language}_with_responses.json'

    if not input_file.exists():
        print(f"Error: {input_file} not found!")
        print("Make sure Romanian responses have been generated.")
        return

    # Check if already done
    final_output = output_dir / f'{language}_evaluated.json'
    if final_output.exists():
        print(f"Romanian already evaluated: {final_output}")
        return

    # Load data
    print(f"\nLoading {language} responses...")
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    print(f"Total entries: {len(data)}")

    # Load API keys
    with open('config.json', 'r') as f:
        config = json.load(f)
    api_keys = config['api_keys'][:5]

    # Distribute entries across 5 APIs
    entries_per_api = len(data) // 5
    remainder = len(data) % 5

    api_assignments = {}
    start_idx = 0

    for api_idx in range(5):
        # Add 1 extra entry to first APIs if there's remainder
        count = entries_per_api + (1 if api_idx < remainder else 0)
        api_assignments[api_idx] = {
            'entries': data[start_idx:start_idx + count],
            'start_idx': start_idx,
            'count': count
        }
        start_idx += count

    print("\nDistribution across APIs:")
    for api_idx, assignment in api_assignments.items():
        print(f"  API {api_idx}: {assignment['count']} entries (indices {assignment['start_idx']}-{assignment['start_idx'] + assignment['count'] - 1})")

    print("\nStarting parallel evaluation...")
    print("="*80)

    # Process in parallel
    all_results = [None] * len(data)  # Pre-allocate results array

    def process_api(api_key, api_idx, entries, start_idx):
        """Process entries for one API"""
        evaluator = RONEvaluator(api_key, api_idx)
        results = evaluator.process_entries(entries, start_idx)

        # Save API-specific results
        api_output = output_dir / f'ron.Latn_api{api_idx}_partial.json'
        with open(api_output, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        print(f"[API {api_idx}] âœ… Completed {len(results)} entries")
        return api_idx, start_idx, results

    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = []

        for api_idx in range(5):
            if api_idx < len(api_keys):
                assignment = api_assignments[api_idx]
                future = executor.submit(
                    process_api,
                    api_keys[api_idx],
                    api_idx,
                    assignment['entries'],
                    assignment['start_idx']
                )
                futures.append(future)

        # Collect results
        for future in futures:
            try:
                api_idx, start_idx, results = future.result()
                # Place results in correct positions
                for i, result in enumerate(results):
                    all_results[start_idx + i] = result
            except Exception as e:
                print(f"Error in API thread: {e}")

    # Merge and save final results
    print("\nMerging results...")

    # Filter out None values (in case of errors)
    final_results = [r for r in all_results if r is not None]

    # Save final merged file
    with open(final_output, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)

    # Clean up partial files
    for api_idx in range(5):
        partial_file = output_dir / f'ron.Latn_api{api_idx}_partial.json'
        if partial_file.exists():
            partial_file.unlink()

    # Generate summary
    refused = sum(1 for d in final_results if d.get('evaluation', {}).get('refused', 0) == 1)
    total_score = sum(
        d.get('evaluation', {}).get('convincing_score', 0) +
        d.get('evaluation', {}).get('specific_score', 0)
        for d in final_results
    )

    summary = {
        'language': language,
        'total_entries': len(final_results),
        'refused_count': refused,
        'refusal_rate': (refused / len(final_results) * 100) if final_results else 0,
        'avg_score': (total_score / len(final_results) / 2) if final_results else 0,
        'timestamp': datetime.now().isoformat()
    }

    summary_file = output_dir / f'{language}_summary.json'
    with open(summary_file, 'w') as f:
        json.dump(summary, f, indent=2)

    print("\n" + "="*80)
    print("âœ… ROMANIAN EVALUATION COMPLETE!")
    print(f"Total entries evaluated: {len(final_results)}")
    print(f"Refusal rate: {summary['refusal_rate']:.1f}%")
    print(f"Average score: {summary['avg_score']:.2f}")
    print(f"\nOutput files:")
    print(f"  - {final_output}")
    print(f"  - {summary_file}")
    print("="*80)


if __name__ == "__main__":
    main()