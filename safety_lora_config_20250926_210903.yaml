# Model
model_name_or_path: swiss-ai/Apertus-70B
attn_implementation: flash-attention-3
dtype: bfloat16

# Dataset
dataset_name: safety_dataset_for_training
dataset_train_split: train
dataset_test_split: validation
dataset_num_proc: 12

# Hyperparameters for safety fine-tuning
learning_rate: 1.0e-4  # Lower LR for safety fine-tuning
gradient_checkpointing: true
num_train_epochs: 2  # More epochs for safety
logging_steps: 10
per_device_train_batch_size: 2  # Smaller batch for 70B model
gradient_accumulation_steps: 8  # Effective batch size = 16

# LoRA Configuration
use_peft: true
lora_r: 16  # Higher rank for complex safety patterns
lora_alpha: 32
lora_dropout: 0.1
lora_target_modules: all-linear

# Sequence length
max_length: 512  # Shorter for safety responses

# Learning rate scheduler
warmup_ratio: 0.1
lr_scheduler_type: cosine_with_min_lr
lr_scheduler_kwargs:
  min_lr_rate: 0.1

# Output & logging
output_dir: ./safety_lora_adapter_20250926_210903
report_to: "none"
seed: 42
save_strategy: steps
save_steps: 500
evaluation_strategy: steps
eval_steps: 100
save_total_limit: 3
load_best_model_at_end: true
